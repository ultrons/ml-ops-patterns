{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6b56b1c7b76"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "976753012196"
   },
   "source": [
    "# Feedback or issues?\n",
    "For any feedback or questions, please open an [issue](https://github.com/googleapis/python-aiplatform/issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c3048cd1427"
   },
   "source": [
    "# Vertex SDK for Python: Big Query Custom Container Training Example\n",
    "To use this Jupyter notebook, copy the notebook to a Google Cloud Notebooks instance and open it. You can run each step, or cell, and see its results. To run a cell, use Shift+Enter. Jupyter automatically displays the return value of the last line in each cell. For more information about running notebooks in Google Cloud Notebook, see the [Google Cloud Notebook guide](https://cloud.google.com/vertex-ai/docs/general/notebooks).\n",
    "\n",
    "This notebook demonstrate how to create a Custom Model using Custom Container Training and a Big Query Dataset. It will require you provide a bucket where the dataset will be stored.\n",
    "\n",
    "Note: you may incur charges for training, prediction, storage or usage of other GCP products in connection with testing this SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpYscdzmf4Gu"
   },
   "source": [
    "# Ensure the following APIs are enabled:\n",
    "- [BigQuery](https://console.cloud.google.com/apis/library/bigquery.googleapis.com?q=BigQuery)\n",
    "- [Cloudbuild](https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com?q=Cloudbuild)\n",
    "- [Container Registry](https://console.cloud.google.com/apis/library/containerregistry.googleapis.com?q=container%20registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOMNWzTbftDr"
   },
   "source": [
    "# Install Vertex SDK for Python\n",
    "\n",
    "\n",
    "After the SDK installation the kernel will be automatically restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be020jY-ftDv"
   },
   "outputs": [],
   "source": [
    "!pip3 uninstall -y google-cloud-aiplatform\n",
    "!pip3 install google-cloud-aiplatform\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "181b681faf5c"
   },
   "source": [
    "### Enter your project and GCS bucket\n",
    "\n",
    "Enter your Project Id in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d61oYG3KftDw"
   },
   "outputs": [],
   "source": [
    "MY_PROJECT = \"pytorch-tpu-nfs\"\n",
    "MY_STAGING_BUCKET = \"gs://automl-samples\"  # bucket should be in same region as ucaip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T1d5uBoftDw"
   },
   "source": [
    "# Copy Big Query Iris Dataset\n",
    "We will make a Big Query dataset and copy Big Query's public iris table to that dataset. For more information about this dataset please visit: https://archive.ics.uci.edu/ml/datasets/iris "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJF047yNftDw"
   },
   "source": [
    "### Make BQ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9yOl-l_oftDx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'pytorch-tpu-nfs:ml_datasets' successfully created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = MY_PROJECT\n",
    "!bq mk {MY_PROJECT}:ml_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xn9TuBZAftDx"
   },
   "source": [
    "### Copy bigquery-public-data.ml_datasets.iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ISFR8nFfftDx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r8adae8c05490196_00000179c0087101_1 ... (0s) Current status: DONE   \n",
      "Table 'bigquery-public-data:ml_datasets.iris' successfully copied to 'pytorch-tpu-nfs:ml_datasets.iris'\n"
     ]
    }
   ],
   "source": [
    "!bq cp -n bigquery-public-data:ml_datasets.iris {MY_PROJECT}:ml_datasets.iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IltwFqKIftDx"
   },
   "source": [
    "# Create Training Container\n",
    "We will create a directory and write all of our container build artifacts into that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "40BkhtMeftDy"
   },
   "outputs": [],
   "source": [
    "CONTAINER_ARTIFACTS_DIR = \"demo-container-artifacts\"\n",
    "\n",
    "!mkdir {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVeG-LPOftDy"
   },
   "source": [
    "### Create Cloudbuild YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4kODuFZCftDy"
   },
   "outputs": [],
   "source": [
    "cloudbuild_yaml = \"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', 'gcr.io/${PROJECT_ID}/test-custom-container', '.' ]\n",
    "images: ['gcr.io/${PROJECT_ID}/test-custom-container']\"\"\"\n",
    "\n",
    "with open(f\"{CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ_GUCtZftDz"
   },
   "source": [
    "### Write Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Rja_jo3rftDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo-container-artifacts/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/Dockerfile\n",
    "\n",
    "# Specifies base image and tag\n",
    "FROM gcr.io/google-appengine/python\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip3 install tensorflow tensorflow-io pyarrow\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY test_script.py /root/test_script.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"test_script.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dfrLShaftDz"
   },
   "source": [
    "### Write entrypoint script to invoke trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "S0jSd8NWftDz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo-container-artifacts/test_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CONTAINER_ARTIFACTS_DIR}/test_script.py\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "import os\n",
    "\n",
    "training_data_uri = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
    "validation_data_uri = os.environ[\"AIP_VALIDATION_DATA_URI\"]\n",
    "test_data_uri = os.environ[\"AIP_TEST_DATA_URI\"]\n",
    "data_format = os.environ[\"AIP_DATA_FORMAT\"]\n",
    "print(\"DEBUG_INFO:\", training_data_uri,validation_data_uri, test_data_uri, data_format)\n",
    "\n",
    "def caip_uri_to_fields(uri):\n",
    "    uri = uri[5:]\n",
    "    project, dataset, table = uri.split('.')\n",
    "    return project, dataset, table\n",
    "\n",
    "feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "target_name = 'species'\n",
    "\n",
    "def transform_row(row_dict):\n",
    "  # Trim all string tensors\n",
    "  trimmed_dict = { column:\n",
    "                  (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                  for (column,tensor) in row_dict.items()\n",
    "                  }\n",
    "  target = trimmed_dict.pop(target_name)\n",
    "\n",
    "  target_float = tf.cond(tf.equal(tf.strings.strip(target), 'versicolor'), \n",
    "                 lambda: tf.constant(1.0),\n",
    "                 lambda: tf.constant(0.0))\n",
    "  return (trimmed_dict, target_float)\n",
    "\n",
    "def read_bigquery(project, dataset, table):\n",
    "  tensorflow_io_bigquery_client = BigQueryClient()\n",
    "  read_session = tensorflow_io_bigquery_client.read_session(\n",
    "      \"projects/\" + project,\n",
    "      project, table, dataset,\n",
    "      feature_names + [target_name],\n",
    "      [dtypes.float64] * 4 + [dtypes.string],\n",
    "      requested_streams=2)\n",
    "\n",
    "  dataset = read_session.parallel_read_rows()\n",
    "  transformed_ds = dataset.map(transform_row)\n",
    "  return transformed_ds\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "training_ds = read_bigquery(*caip_uri_to_fields(training_data_uri)).shuffle(10).batch(BATCH_SIZE)\n",
    "eval_ds = read_bigquery(*caip_uri_to_fields(validation_data_uri)).batch(BATCH_SIZE)\n",
    "test_ds = read_bigquery(*caip_uri_to_fields(test_data_uri)).batch(BATCH_SIZE)\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in feature_names:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "Dense = tf.keras.layers.Dense\n",
    "model = tf.keras.Sequential(\n",
    "  [\n",
    "    feature_layer,\n",
    "      Dense(16, activation=tf.nn.relu),\n",
    "      Dense(8, activation=tf.nn.relu),\n",
    "      Dense(4, activation=tf.nn.relu),\n",
    "      Dense(1, activation=tf.nn.sigmoid),\n",
    "  ])\n",
    "\n",
    "# Compile Keras model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    metrics=['accuracy'],\n",
    "    optimizer='adam')\n",
    "\n",
    "model.fit(training_ds, epochs=5, validation_data=eval_ds)\n",
    "\n",
    "print(model.evaluate(test_ds))\n",
    "\n",
    "tf.saved_model.save(model, os.environ[\"AIP_MODEL_DIR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LYlV4D2ftD0"
   },
   "source": [
    "### Build Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9tGdX7B_ftD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 3.2 KiB before compression.\n",
      "Uploading tarball of [demo-container-artifacts] to [gs://pytorch-tpu-nfs_cloudbuild/source/1622424671.900764-794378dddbbb4a4383b33f0acb23e7ff.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/pytorch-tpu-nfs/builds/4750a3e4-f1dc-493a-82fe-49c3b14e0d3b].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/4750a3e4-f1dc-493a-82fe-49c3b14e0d3b?project=64701051322].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4750a3e4-f1dc-493a-82fe-49c3b14e0d3b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://pytorch-tpu-nfs_cloudbuild/source/1622424671.900764-794378dddbbb4a4383b33f0acb23e7ff.tgz#1622424672225601\n",
      "Copying gs://pytorch-tpu-nfs_cloudbuild/source/1622424671.900764-794378dddbbb4a4383b33f0acb23e7ff.tgz#1622424672225601...\n",
      "/ [1 files][  1.5 KiB/  1.5 KiB]                                                \n",
      "Operation completed over 1 objects/1.5 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/5 : FROM gcr.io/google-appengine/python\n",
      "latest: Pulling from google-appengine/python\n",
      "Digest: sha256:fc9d9f429bf75b8369f7894b7eb24098418050557b889f4b02aa4c93fdb5e161\n",
      "Status: Downloaded newer image for gcr.io/google-appengine/python:latest\n",
      " ---> 96b4d9af6594\n",
      "Step 2/5 : WORKDIR /root\n",
      " ---> Running in eb985db88883\n",
      "Removing intermediate container eb985db88883\n",
      " ---> 79a831c6433e\n",
      "Step 3/5 : RUN pip3 install tensorflow tensorflow-io pyarrow\n",
      " ---> Running in 5929df57bd93\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "Collecting tensorflow-io\n",
      "  Downloading tensorflow_io-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-4.0.1-cp37-cp37m-manylinux2014_x86_64.whl (21.8 MB)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.17.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting six~=1.15.0\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.18.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.18.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.5 MB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting setuptools>=41.0.0\n",
      "  Downloading setuptools-57.0.0-py3-none-any.whl (821 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.30.1-py2.py3-none-any.whl (146 kB)\n",
      "Collecting cached-property; python_version < \"3.8\"\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<5,>=3.0.2\n",
      "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-4.3.1-py3-none-any.whl (16 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: wrapt, termcolor\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=65543 sha256=f6a346e702e564e837b123f0d3bda3b105dbdaa6882e08ae3c1be45ffae251e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4001 sha256=0bb09655f00f64bb57dba29a257729db2a44efa3ee04c31d2b8d35b026bddb96\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built wrapt termcolor\n",
      "Installing collected packages: wrapt, typing-extensions, six, protobuf, termcolor, numpy, opt-einsum, grpcio, gast, keras-nightly, tensorflow-estimator, wheel, absl-py, flatbuffers, urllib3, certifi, idna, chardet, requests, setuptools, tensorboard-data-server, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, tensorboard-plugin-wit, zipp, importlib-metadata, markdown, tensorboard, keras-preprocessing, astunparse, google-pasta, cached-property, h5py, tensorflow, tensorflow-io-gcs-filesystem, tensorflow-io, pyarrow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.31.1\n",
      "    Uninstalling wheel-0.31.1:\n",
      "      Successfully uninstalled wheel-0.31.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 40.2.0\n",
      "    Uninstalling setuptools-40.2.0:\n",
      "      Successfully uninstalled setuptools-40.2.0\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.2 certifi-2021.5.30 chardet-4.0.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.30.1 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 idna-2.10 importlib-metadata-4.3.1 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.17.1 pyarrow-4.0.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-57.0.0 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 tensorflow-io-0.18.0 tensorflow-io-gcs-filesystem-0.18.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.5 werkzeug-2.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\n",
      "\u001b[91mWARNING: You are using pip version 20.2.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/opt/python3.7/bin/python3.7 -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 5929df57bd93\n",
      " ---> 1dbab7eeb776\n",
      "Step 4/5 : COPY test_script.py /root/test_script.py\n",
      " ---> 40c1dedb55b4\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"test_script.py\"]\n",
      " ---> Running in b5379b976941\n",
      "Removing intermediate container b5379b976941\n",
      " ---> 64aca184aa4d\n",
      "Successfully built 64aca184aa4d\n",
      "Successfully tagged gcr.io/pytorch-tpu-nfs/test-custom-container:latest\n",
      "PUSH\n",
      "Pushing gcr.io/pytorch-tpu-nfs/test-custom-container\n",
      "The push refers to repository [gcr.io/pytorch-tpu-nfs/test-custom-container]\n",
      "d1cca20dee50: Preparing\n",
      "4f1f6c931ab4: Preparing\n",
      "3ec03be1bb38: Preparing\n",
      "c5f91450ff31: Preparing\n",
      "d8a99d217f22: Preparing\n",
      "88cace8eb32b: Preparing\n",
      "ae3630d8a5da: Preparing\n",
      "f4abe2e82c92: Preparing\n",
      "a43e6a3baa91: Preparing\n",
      "41e2d890e349: Preparing\n",
      "84ff92691f90: Preparing\n",
      "6071362d21ac: Preparing\n",
      "965eb7efdb65: Preparing\n",
      "f4abe2e82c92: Waiting\n",
      "a43e6a3baa91: Waiting\n",
      "41e2d890e349: Waiting\n",
      "84ff92691f90: Waiting\n",
      "6071362d21ac: Waiting\n",
      "965eb7efdb65: Waiting\n",
      "88cace8eb32b: Waiting\n",
      "ae3630d8a5da: Waiting\n",
      "d1cca20dee50: Pushed\n",
      "c5f91450ff31: Pushed\n",
      "3ec03be1bb38: Pushed\n",
      "88cace8eb32b: Pushed\n",
      "a43e6a3baa91: Pushed\n",
      "41e2d890e349: Pushed\n",
      "84ff92691f90: Layer already exists\n",
      "d8a99d217f22: Pushed\n",
      "6071362d21ac: Pushed\n",
      "f4abe2e82c92: Pushed\n",
      "965eb7efdb65: Pushed\n",
      "ae3630d8a5da: Pushed\n",
      "4f1f6c931ab4: Pushed\n",
      "latest: digest: sha256:8106b948c29916c0210ad3ae432e44ab7ebb76ab1def48f833adf2735cd670ed size: 3043\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                         IMAGES                                                  STATUS\n",
      "4750a3e4-f1dc-493a-82fe-49c3b14e0d3b  2021-05-31T01:31:12+00:00  4M52S     gs://pytorch-tpu-nfs_cloudbuild/source/1622424671.900764-794378dddbbb4a4383b33f0acb23e7ff.tgz  gcr.io/pytorch-tpu-nfs/test-custom-container (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --config {CONTAINER_ARTIFACTS_DIR}/cloudbuild.yaml {CONTAINER_ARTIFACTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf0pugbvftD1"
   },
   "source": [
    "# Run Custom Container Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ee691569d8d"
   },
   "source": [
    "## Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the *client* for Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vEEr62NUftD1"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=MY_PROJECT, staging_bucket=MY_STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "736ddff8408b"
   },
   "source": [
    "# Create a Managed Tabular Dataset from Big Query Dataset\n",
    "\n",
    "This section will create a managed Tabular dataset from the iris Big Query table we copied above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oBdOv6lWftD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.datasets.dataset:Creating TabularDataset\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:Create TabularDataset backing LRO: projects/64701051322/locations/us-central1/datasets/1761268094115774464/operations/5289327679009456128\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:TabularDataset created. Resource name: projects/64701051322/locations/us-central1/datasets/1761268094115774464\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:To use this TabularDataset in another session:\n",
      "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.TabularDataset('projects/64701051322/locations/us-central1/datasets/1761268094115774464')\n"
     ]
    }
   ],
   "source": [
    "ds = aiplatform.TabularDataset.create(\n",
    "    display_name=\"bq_iris_dataset\", bq_source=f\"bq://{MY_PROJECT}.ml_datasets.iris\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee242cc1f74c"
   },
   "source": [
    "# Launch a Training Job to Create a Model\n",
    "\n",
    "We will train a model with the container we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRGrFdxOftD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://automl-samples/aiplatform-custom-training-2021-05-31-01:36:10.481 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/144106391982833664?project=64701051322\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/64701051322/locations/us-central1/trainingPipelines/144106391982833664 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=\"train-bq-iris\",\n",
    "    container_uri=f\"gcr.io/{MY_PROJECT}/test-custom-container:latest\",\n",
    "    model_serving_container_image_uri=\"gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-2:latest\",\n",
    ")\n",
    "model = job.run(\n",
    "    ds,\n",
    "    replica_count=1,\n",
    "    model_display_name=\"bq-iris-model\",\n",
    "    bigquery_destination=f\"bq://{MY_PROJECT}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7fa9b59f919"
   },
   "source": [
    "# Deploy Your Model\n",
    "\n",
    "Deploy your model, then wait until the model FINISHES deployment before proceeding to prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEg2IDwPftD2"
   },
   "outputs": [],
   "source": [
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dbd6c650a03"
   },
   "source": [
    "# Predict on the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKVhGB1PftD2"
   },
   "outputs": [],
   "source": [
    "endpoint.predict(\n",
    "    [{\"sepal_length\": 5.1, \"sepal_width\": 2.5, \"petal_length\": 3.0, \"petal_width\": 1.1}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_Platform_(Unified)_SDK_BigQuery_Custom_Container_Training.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
