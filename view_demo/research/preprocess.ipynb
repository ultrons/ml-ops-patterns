{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from view_demo.utils import run_and_save, get_project_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRCROOT='../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {SRCROOT}/preprocess/create_dataset.py\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "import tempfile\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--raw-dataset', dest='raw_dataset',\n",
    "                    default=\"None\", type=str, help='GCS Path to the raw dataset')\n",
    "parser.add_argument('--project-id', dest='project_id',\n",
    "                    default=get_project_id(), type=str, help='Project ID')\n",
    "parser.add_argument('--target-dataset', dest='target_dataset',\n",
    "                    default=\"view_dataset\", type=str, help='Name of the BQ Dataset where preprocessed data will be pushed')\n",
    "parser.add_argument('--target-table', dest='target_table',\n",
    "                    default=\"weather_time_series\", type=str, help='Name of the table under' \n",
    "                    'the BQ Dataset where preprocessed data will be pushed')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Read the dataset into a dataframe\n",
    "csv_path = args.raw_dataset\n",
    "dataset_id = args.target_dataset\n",
    "table_id = args.target_table\n",
    "project_id = args.project_id\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to hourly dataset\n",
    "# slice [start:stop:step], starting from index 5 take every 6th record.\n",
    "df = df[5::6]\n",
    "\n",
    "\n",
    "# Clean Data\n",
    "wv = df['wv (m/s)']\n",
    "bad_wv = wv == -9999.0\n",
    "wv[bad_wv] = 0.0\n",
    "\n",
    "max_wv = df['max. wv (m/s)']\n",
    "bad_max_wv = max_wv == -9999.0\n",
    "max_wv[bad_max_wv] = 0.0\n",
    "\n",
    "# The above inplace edits are reflected in the DataFrame\n",
    "df['wv (m/s)'].min()\n",
    "\n",
    "\n",
    "# Rename Columns to comply with BQ\n",
    "df.rename(columns={\n",
    "    'p (mbar)': 'p__mbar', \n",
    "    'T (degC)': 'T__degC',\n",
    "    'Tpot (K)': 'Tpot__K', \n",
    "    'Tdew (degC)': 'Tdew__degC', \n",
    "    'rh (%)': 'rh__percent',\n",
    "    'VPmax (mbar)': 'VPmax__mbar' ,\n",
    "    'VPact (mbar)': 'VPact__mbar',\n",
    "    'VPdef (mbar)': 'VPdef__mbar',\n",
    "    'sh (g/kg)': 'sh__g_per_kg',\n",
    "    'H2OC (mmol/mol)': 'H2OC__mmol_per_mol',\n",
    "    'rho (g/m**3)': 'rho__gm_per_cubic_m',\n",
    "    'max Wx': 'max_Wx',\n",
    "    'max Wy': 'max_Wy', \n",
    "    'Day sin': 'Day_sin', \n",
    "    'Day cos': 'Day_cos', \n",
    "    'Year sin': 'Year_sin', \n",
    "    'Year cos': 'Year_cos'\n",
    "    \n",
    "}, inplace=True)\n",
    "\n",
    "# Write to BQ\n",
    "client = bigquery.Client(location=\"us-central1\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))\n",
    "\n",
    "# The project defaults to the Client's project if not specified.\n",
    "try:\n",
    "    dataset = client.get_dataset(dataset_id)  # Make an API request.\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except NotFound:\n",
    "    print(\"Dataset {} is not found, Creating..\".format(dataset_id))\n",
    "    dataset = client.create_dataset(dataset_id)\n",
    "    \n",
    "table_ref = dataset.table(table_id)\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    destination=table_ref,\n",
    "    autodetect=True,\n",
    ")\n",
    "# Overwrite the table if already exists\n",
    "job_config.write_disposition = 'WRITE_TRUNCATE'\n",
    "\n",
    "job = client.load_table_from_dataframe(df, table_ref, location=\"us-central1\")\n",
    "job.result()  # Waits for table load to complete.\n",
    "print(\"Loaded dataframe to {}\".format(table_ref.path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
